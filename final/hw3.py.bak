#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""final.py: Data mining final project: Data classification and evaluation."""

__author__ = "Dan Collins"

import math
import random
import numpy 
import csv
import operator
import copy 
import warnings
from tabulate import tabulate

class DataClassification:
    """Class which contains methods to classify and evaluate a data set."""

        
    def classify_mpg_lr(self, trainingSet, index, x):
        """Classifies an x value according to a linear regression \
           performed on a training set."""
        #Get the list of values to be compared with mpg (weight for this program)
        xs = self.get_column_as_floats(trainingSet, index)
        
        #Get the list of mpg values
        ys = self.get_column_as_floats(trainingSet, 0)
        
        #Calculate linear regression values for mpg and given variable (weight)
        m, b = self.calculate_least_squares_lr(xs, ys)
        
        #Calculate the predicted value for the given x (a weight value)
        y = m * float(x) + b
        
        #Classify based on department of energy ratings
        classification = self.classify_mpg_DoE(y)
        
        return classification
        
    def test_random_instances_step1(self, seed):
        """Test step 1 linear regression classifier on 5 random instances."""
        print '==========================================='
        print 'STEP 1: Linear Regression MPG Classifier'
        print '==========================================='
        
        table = copy.deepcopy(self.__table)
        weights = self.get_column_as_floats(table, 4)
                    
        testValues = []
        printIndices = []
        for rand_i in seed:
            val = weights[rand_i]
            
            printIndices.append(rand_i)
            testValues.append(val)
                        
        for i in range(len(testValues)):
            classification = self.classify_mpg_lr(table, 4, testValues[i])
            instance = table[printIndices[i]]
            actual = self.classify_mpg_DoE(instance[0])
            print '    instance:', ", ".join(instance)
            print '    class:', str(classification) + ',', 'actual:', actual
        print
            
    def test_random_instances_step2(self, seed):
        """Test step 2 K-NN classifier on 5 random instances."""
        print '==========================================='
        print 'STEP 2: k=5 Nearest Neighbor MPG Classifier'
        print '==========================================='

        k = 5
        classIndex = 0 # mpg
        indices = [1, 4, 5] # cylinders, weight, acceleration
        table = self.normalize_table(self.__table, indices)    
        trainingSet = table # training and test are same for this step
        testSet = table

        # classify k=5 random instances
        for rand_i in seed:
            instance = testSet[rand_i]
            origInstance = self.__table[rand_i]
            classification = self.k_nn_classifier(trainingSet, \
                             indices, instance, k, classIndex)

            actual = self.classify_mpg_DoE(instance[0])
            print '    instance:', ", ".join(origInstance)
            print '    class:', str(classification) + ',', 'actual:', actual
        print
    
    def test_random_instances_step3(self, seed):
        """Classify MPG with Naive Bayes I and II."""
        print '==========================================='
        print 'STEP 3: Naive Bayes MPG Classifiers'
        print '==========================================='

        attrIndices = [1, 4, 6] # cylinders, weight, year
        classIndex = 0 # mpg
        table = copy.deepcopy(self.__table)
        table = self.categorize_weight(table)
        for row in table:
            row[0] = str(self.classify_mpg_DoE(row[0]))

        print 'Naive Bayes I:'
        for rand_i in seed:
            instance = table[rand_i]
            origInstance = self.__table[rand_i]
            actual = instance[0]
            classification = self.naive_bayes_i(instance, classIndex, attrIndices, table)
            print '    instance:', ", ".join(origInstance)
            print '    class:', str(int(classification)) + ',', 'actual:', actual

        print 'Naive Bayes II:'
        for rand_i in seed:
            instance = table[rand_i]
            origInstance = self.__table[rand_i]
            actual = instance[0]
            classification = self.naive_bayes_ii(instance, classIndex, attrIndices, table)
            print '    instance:', ", ".join(origInstance)
            print '    class:', str(int(classification)) + ',', 'actual:', actual
        print

    def evaluate_classifiers_step4(self):
        """Evaluates predictive accuracy of classifiers used so far using 
           predictive accuracy and standard error."""
        print '==========================================='
        print 'STEP 4: Predictive Accuracy'
        print '==========================================='
        k = 10
        
        print '    Random Subsample (k=10, 2:1 Train/Test)'
        predacc_lr, stderr_lr     = self.accuracy(k, 0, 0)
        print '        Linear Regression      : p =', predacc_lr, '+-', stderr_lr 
        predacc_nbi, stderr_nbi   = self.accuracy(k, 1, 0)
        print '        Naive Bayes I          : p =', predacc_nbi, '+-', stderr_nbi 
        predacc_nbii, stderr_nbii = self.accuracy(k, 2, 0)
        print '        Naive Bayes II         : p =', predacc_nbii, '+-', stderr_nbii 
        predacc_knn, stderr_knn   = self.accuracy(k, 3, 0)
        print '        Top-5 Nearest Neighbor : p =', predacc_knn, '+-', stderr_knn 
        
        print '    Stratified 10-Fold Cross Validation'
        predacc_lr, stderr_lr     = self.accuracy(k, 0, 1)
        print '        Linear Regression      : p =', predacc_lr, '+-', stderr_lr 
        predacc_nbi, stderr_nbi   = self.accuracy(k, 1, 1)
        print '        Naive Bayes I          : p =', predacc_nbi, '+-', stderr_nbi 
        predacc_nbii, stderr_nbii = self.accuracy(k, 2, 1)
        print '        Naive Bayes II         : p =', predacc_nbii, '+-', stderr_nbii 
        predacc_knn, stderr_knn   = self.accuracy(k, 3, 1)
        print '        Top-5 Nearest Neighbor : p =', predacc_knn, '+-', stderr_knn 
        print
        
    def classify_survivors(self):
        """Attempts to predict if a passenger from titanic.txt survived using both
           K-NN and Naive Bayes I."""
        # pop 1st row as class names
        attrNames = self.__table.pop(0)
        attrIndices = [0, 1, 2]
        classIndex = 3
        k = 10 

        print '==========================================='
        print 'STEP 6: Classify Titanic Survivors'
        print '==========================================='
        print '    Hold tight, we\'re calculating. . .'        

        predAccKNN, predAccNBI = [], []
        totalKNN, totalNBI, totalActual = [], [], []
        for i in range(k):
            actual = []
            result_knn, result_nbi = [], []
            # partition the data with kfold
            trainingSet, testSet = \
                self.k_cross_fold_partition(self.__table, k, classIndex, i)
            for instance in testSet:
                # call two different classifiers
                actual.append(instance[classIndex])
                result_knn.append(self.k_nn_classifier(trainingSet, attrIndices, instance, \
                                  100, classIndex))
                result_nbi.append(self.naive_bayes_i(instance, classIndex, attrIndices, \
                                  trainingSet))

            # calculate predictive accuracy 
            predAccKNN.append(len(testSet) * \
                self.calculate_predacc(result_knn, actual, len(testSet)))
            predAccNBI.append(len(testSet) * \
                self.calculate_predacc(result_nbi, actual, len(testSet)))
            
            # keep master list of expected vs actual for confusion matrices
            totalKNN += result_knn
            totalNBI += result_nbi
            totalActual += actual      

        avgPredAccKNN = round(sum(predAccKNN) / len(self.__table), 2)
        stderrKNN = self.calculate_std_error(avgPredAccKNN, len(testSet))
        avgPredAccNBI = round(sum(predAccNBI) / len(self.__table), 2)
        stderrNBI = self.calculate_std_error(avgPredAccNBI, len(testSet))
        
        # Calculate the interval with probability 0.95
        zCLStderrKNN = 1.96 * stderrKNN
        zCLStderrNBI = 1.96 * stderrNBI

        print
        print '100-Nearest Neighbor (Stratified 10-Fold Cross Validation)'
        print 'Predictive Accuracy', avgPredAccKNN, '+-', zCLStderrKNN
        print 'Confusion matrix:'
        KNNconfusionMatrix = self.create_confusion_matrix_titanic(totalKNN, totalActual)
        print tabulate(KNNconfusionMatrix)
        print
        
        print 'Naive Bayes I (Stratified 10-Fold Cross Validation)'
        print 'Predictive Accuracy:', avgPredAccNBI, '+-', zCLStderrNBI
        print 'Confusion matrix:'
        NBConfusionMatrix = self.create_confusion_matrix_titanic(totalNBI, totalActual)
        print tabulate(NBConfusionMatrix)
        # done, drink beer

def main():
    """Entry point for hw3. Creates an object for each data set and then operates on those
       objects to both classify and evaluate the performance of the various classifiers."""
    warnings.simplefilter("error")
    
    a = DataClassification("auto-data.txt")
    # generate seed - 5 random row indices from table
    seed = []
    for i in range(5):
        rand = random.randint(0, a.get_table_len() - 1)
        while i in seed:
            rand = random.randint(0, a.get_table_len() - 1)
        seed.append(rand)

    a.test_random_instances_step1(seed)
    a.test_random_instances_step2(seed)
    a.test_random_instances_step3(seed)
    a.evaluate_classifiers_step4()
    a.generate_confusion_matrices()

    t = DataClassification("titanic.txt")
    t.classify_survivors()

if __name__ == "__main__":
    main()
    
